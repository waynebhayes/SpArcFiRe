{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65604d66-971e-49eb-9cc8-c879435f15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabs I have open\n",
    "# https://xgboost.readthedocs.io/en/stable/python/examples/multioutput_regression.html#sphx-glr-python-examples-multioutput-regression-py\n",
    "# https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor\n",
    "# https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "# https://xgboost.readthedocs.io/en/stable/prediction.html\n",
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py\n",
    "# https://machinelearningmastery.com/xgboost-for-regression/\n",
    "# https://machinelearningmastery.com/regression-metrics-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99eb61ae-51d7-43e4-b45c-f3f2ecf75804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas\n",
    "#!pip install graphviz\n",
    "#!pip install xgboost\n",
    "#!pip install hyperopt\n",
    "#!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9c70e6-141b-4364-98f8-ef426bf45352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, SCORERS\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from hyperopt import fmin, Trials, hp, tpe, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfba9b5d-d601-44fe-a091-5e69955b1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kstest, anderson\n",
    "\n",
    "from math import ceil\n",
    "import itertools\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#import graphviz\n",
    "import ssl\n",
    "from glob import glob\n",
    "\n",
    "#from annoy import AnnoyIndex\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "from shutil import copy2\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c63468-e491-46a7-bb65-874600bd6187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "from IPython import get_ipython\n",
    "def in_notebook():\n",
    "    ip = get_ipython()\n",
    "    \n",
    "    if ip:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81ace0e-098d-4ad4-928a-08fc91ac7e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join as pj\n",
    "\n",
    "_HOME_DIR = os.path.expanduser(\"~\")\n",
    "if in_notebook():\n",
    "    _SPARCFIRE_DIR = pj(_HOME_DIR, \"sparcfire_matt\") \n",
    "    _MODULE_DIR    = pj(_SPARCFIRE_DIR, \"GalfitModule\")\n",
    "else:\n",
    "    try:\n",
    "        _SPARCFIRE_DIR = os.environ[\"SPARCFIRE_HOME\"]\n",
    "        _MODULE_DIR = pj(_SPARCFIRE_DIR, \"GalfitModule\")\n",
    "    except KeyError:\n",
    "        print(\"SPARCFIRE_HOME is not set. Please run 'setup.bash' inside SpArcFiRe directory if not done so already.\")\n",
    "        print(\"Running on the assumption that GalfitModule is in your home directory... (if not this will fail and quit!)\") \n",
    "        _MODULE_DIR = pj(_HOME_DIR, \"GalfitModule\")\n",
    "    \n",
    "sys.path.append(_MODULE_DIR)\n",
    "\n",
    "from Classes.Components import *\n",
    "from Classes.Containers import *\n",
    "from Classes.FitsHandlers import *\n",
    "from Functions.helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4fa94-27dd-4ecd-8587-2dd591430aff",
   "metadata": {},
   "source": [
    "#!jupyter nbconvert --to script testing_xgboost.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84028970-8f3a-433f-a70e-ec9df5e20969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Force >python 3.10 for various compatabilities\n",
    "    out_str = \"\\t Python3.10 or greater required! Exitting without generating feedmes...\"\n",
    "    assert sys.version_info >= (3, 10), out_str\n",
    "    \n",
    "    cwd = absp(os.getcwd()) # Doesn't work *in* notebook\n",
    "    old_cwd = absp(cwd) # Strings are immutable\n",
    "    \n",
    "    username = os.environ[\"USER\"]\n",
    "    \n",
    "    USAGE = f\"\"\"USAGE:\n",
    "\n",
    "    python3 ./{sys.argv[0]} [OPTION] [[RUN-DIRECTORY] IN-DIRECTORY TMP-DIRECTORY OUT-DIRECTORY]\n",
    "    \n",
    "    OPTIONS => [-v | --verbose]\n",
    "\n",
    "    This script is used to train XGBoost to feed better input to GALFIT via SpArcFiRe. \n",
    "    By default, it runs from the RUN (or current) directory and uses the\n",
    "    '-in' '-tmp' and '-out' directories as specified or otherwise defaults to \n",
    "    'sparcfire-in', 'sparcfire-tmp', 'sparcfire-out'. \n",
    "\n",
    "    Please do not specify symlinks for the above, they discomfort the programmer.\n",
    "    \"\"\"\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description = USAGE)\n",
    "    \n",
    "    parser.add_argument('-v', '--verbose',\n",
    "                        dest     = 'verbose', \n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Verbose output for all bash commands in control script.'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(dest     = 'out_path',\n",
    "                        action   = 'store',\n",
    "                        type     = str,\n",
    "                        help     = \"[OUT-DIRECTORY] from SpArcFiRe. \\\n",
    "                                    SpArcFiRe directories should follow -in, -tmp, out or this probably won't work.\"\n",
    "                       )\n",
    "    \n",
    "    if not in_notebook():\n",
    "        args              = parser.parse_args() # Using vars(args) will call produce the args as a dict\n",
    "        \n",
    "        verbose           = args.verbose\n",
    "        capture_output    = not args.verbose\n",
    "        \n",
    "        sparc_out_dir = args.out_path\n",
    "            \n",
    "    else:\n",
    "        verbose = False\n",
    "        capture_output = True\n",
    "        \n",
    "        cwd = cwd.replace(\"ics-home\", username)\n",
    "        sparc_out_dir = pj(_HOME_DIR, \"run2_1000_galfit\", \"sparcfire-out\") #pj(cwd, \"sparcfire-out\")\n",
    "        \n",
    "        sys.path.append(pj(_HOME_DIR, \".local\", \"bin\"))\n",
    "        \n",
    "    # Making these absolute paths\n",
    "    cwd     = absp(cwd)\n",
    "    #in_dir  = absp(in_dir)\n",
    "    #tmp_dir = absp(tmp_dir)\n",
    "    sparc_out_dir = absp(sparc_out_dir)\n",
    "    \n",
    "    # Changing to specified working dir\n",
    "    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1814091a-49d0-48c9-a599-42799d5c956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    png_tiled_dir  = pj(cwd, \"labeled_png_out\")\n",
    "    good_labeled_dir  = pj(png_tiled_dir, \"good\")\n",
    "\n",
    "    inputs_dir = pj(cwd, \"galfit_inputs\")\n",
    "    good_inputs_dir = pj(inputs_dir, \"good\")\n",
    "    not_good_inputs_dir = pj(inputs_dir, \"not_good\")\n",
    "\n",
    "    outputs_dir = pj(cwd, \"galfit_outputs\")\n",
    "    good_outputs_dir = pj(outputs_dir, \"good\")\n",
    "    not_good_outputs_dir = pj(outputs_dir, \"not_good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2668f0-fcdf-4112-81ae-5dfe6b96936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to get organized\n",
    "def generate_labels_from_imgs(all_galaxies_path, good_folder_path):\n",
    "    \n",
    "    # good galaxies are named ###_combined.png\n",
    "    # inputs are named ###.in\n",
    "    \n",
    "    good_galaxy_names = list(map(lambda i: i.split(\"_\")[0], os.listdir(good_folder_path)))\n",
    "    all_galaxy_names = [os.path.basename(i) for i in os.listdir(all_galaxies_path) if os.path.basename(i).startswith(\"123\")]\n",
    "    \n",
    "    label_dict = {i:(1 if i in good_galaxy_names else 0) for i in all_galaxy_names}\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379b13f2-b2c3-4008-af41-d1fd17fe997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_files(label_dict, sparcfire_out_dir, **kwargs):\n",
    "    \n",
    "    labels_top_dir  = kwargs.get(\"labels_top_dir\", os.getcwd())\n",
    "    good_name       = kwargs.get(\"good_name\", \"good\")\n",
    "    not_good_name   = kwargs.get(\"not_good_name\", \"not_good\")\n",
    "    label_dump_path = kwargs.get(\"label_dump_path\", pj(labels_top_dir, \"labeled_galaxies.json\"))\n",
    "    \n",
    "    temp_dict = deepcopy(label_dict)\n",
    "    for gname, label in temp_dict.items():\n",
    "        if label:\n",
    "            label_name = good_name\n",
    "        else:\n",
    "            label_name = not_good_name\n",
    "        \n",
    "        # We will extract feedme from output fits file\n",
    "        # There should always be a generated output (because labeled) but just in case...\n",
    "        # Copy that first so the except catches before trying to copy the input that way we don't have a mismatch\n",
    "        try:\n",
    "            #copy2(pj(sparcfire_out_dir, gname, f\"{gname}_galfit_out.fits\"), pj(labels_top_dir, \"galfit_outputs\", label_name))\n",
    "            #copy2(pj(sparcfire_out_dir, gname, f\"{gname}.in\"), pj(labels_top_dir, \"galfit_inputs\", label_name))\n",
    "            copy2(pj(sparcfire_out_dir, gname, f\"{gname}_out.fits\"), pj(labels_top_dir, \"galfit_outputs\", label_name, f\"{gname}_galfit_out.fits\"))\n",
    "            copy2(pj(sparcfire_out_dir, gname, \"autogen_feedme_galfit.in\"), pj(labels_top_dir, \"galfit_inputs\", label_name, f\"{gname}.in\"))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No output found for {gname}, continuing to copy...\")\n",
    "            label_dict.pop(gname)\n",
    "            \n",
    "    with open(label_dump_path, \"w\") as lg:\n",
    "        json.dump(label_dict, lg)\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4825f8ec-a2b6-463b-a5d6-15630fa85648",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    label_json = pj(cwd, 'labeled_galaxies.json')\n",
    "    \n",
    "    if exists(label_json):\n",
    "        label_dict = json.load(open(label_json, 'r'))\n",
    "    else:\n",
    "        label_dict = generate_labels_from_imgs(sparc_out_dir, good_labeled_dir)\n",
    "        label_dict = organize_files(label_dict, pj(_HOME_DIR, \"run2_1000_galfit\", \"sparcfire-out\"), label_dump_path = label_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6528c485-d5d8-4675-8660-2b229f39ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabeledModel(OutputFits):\n",
    "#     def __init__(self, \n",
    "#                  filepath = \"\",\n",
    "#                  label = 0,\n",
    "#                  **kwargs\n",
    "#                 ):\n",
    "        \n",
    "#         OutputFits.__init__(self, filepath)\n",
    "        \n",
    "#         self.label = label        \n",
    "#         self.df    = self.feedme.to_pandas(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f57399-5351-49f8-b88e-b6005afffbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Non-parallelized\n",
    "# def build_df(label_dict, label_dirs, file_suffix: str):\n",
    "    \n",
    "#     out_df = pd.DataFrame()\n",
    "    \n",
    "#     for gname, label in label_dict.items():\n",
    "\n",
    "#         # 0 bad, 1 good\n",
    "#         if label == 0:\n",
    "#             input_dir = label_dirs[0]\n",
    "#             #str_label = \"not_good\"\n",
    "#         else:\n",
    "#             input_dir = label_dirs[1]\n",
    "#             #str_label = \"good\"\n",
    "\n",
    "#         #gal_dict, param_names = #galfit_param_grab(pj(input_dir, gname + file_suffix))\n",
    "# #         if not param_names: continue\n",
    "        \n",
    "# #         for i, param in enumerate(param_names):\n",
    "# #             if param in param_names[:i]:\n",
    "# #                 param += \"\"\n",
    "\n",
    "#         #gname_df = flatten_to_pandas(gal_dict, param_names, gname)\n",
    "#         path_to_output = pj(input_dir, f\"{gname}{file_suffix}\")\n",
    "        \n",
    "#         if file_suffix.endswith(\".fits\"):\n",
    "#             feedme = OutputFits(path_to_output).feedme\n",
    "#             gname_df = feedme.to_pandas()\n",
    "            \n",
    "#             region_to_fit = feedme.header.region_to_fit\n",
    "#             gname_df[\"crop_rad\"] = region_to_fit[1] - region_to_fit[0]\n",
    "            \n",
    "#         elif file_suffix.endswith(\".in\"):\n",
    "#             feedme = FeedmeContainer(path_to_feedme = path_to_output)\n",
    "#             feedme.from_file(path_to_output)\n",
    "#             gname_df = feedme.to_pandas()\n",
    "            \n",
    "#             region_to_fit = feedme.header.region_to_fit\n",
    "#             gname_df[\"crop_rad\"] = region_to_fit[1] - region_to_fit[0]\n",
    "            \n",
    "#         gname_df[\"label\"] = label #str_label\n",
    "#         # TODO: Put this rename function in a to_pandas() function of OutputFits\n",
    "#         # Make note in documentation: name is incoherent in everything but an outputfits\n",
    "#         # since a feedme does not necessarily imply a galaxy\n",
    "#         # That being said, make an optional gname parameter in feedme\n",
    "#         gname_df.rename(index = {0:gname}, inplace = True)\n",
    "#         out_df = pd.concat([out_df, gname_df])\n",
    "\n",
    "#     # For the input galaxies, we have a lot of held values, these are uneccessary\n",
    "#     # https://stackoverflow.com/a/39658662\n",
    "#     nunique = out_df.nunique()\n",
    "#     cols_to_drop = nunique[nunique == 1].index\n",
    "#     out_df.drop(columns = cols_to_drop, inplace = True)\n",
    "    \n",
    "#     return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f658f6b4-c4b0-4d5e-8066-7355885eed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(gname, label, count, label_dirs, file_suffix: str):\n",
    "    \n",
    "    if not count % 100:\n",
    "        print(gname, count)\n",
    "        \n",
    "    # 0 bad, 1 good\n",
    "    if label == 0:\n",
    "        input_dir = label_dirs[0]\n",
    "        #str_label = \"not_good\"\n",
    "    else:\n",
    "        input_dir = label_dirs[1]\n",
    "        #str_label = \"good\"\n",
    "\n",
    "    #gal_dict, param_names = #galfit_param_grab(pj(input_dir, gname + file_suffix))\n",
    "#         if not param_names: continue\n",
    "\n",
    "#         for i, param in enumerate(param_names):\n",
    "#             if param in param_names[:i]:\n",
    "#                 param += \"\"\n",
    "\n",
    "    #gname_df = flatten_to_pandas(gal_dict, param_names, gname)\n",
    "    path_to_output = pj(input_dir, f\"{gname}{file_suffix}\")\n",
    "\n",
    "    if file_suffix.endswith(\".fits\"):\n",
    "        feedme = OutputFits(path_to_output).feedme\n",
    "        gname_df = feedme.to_pandas()\n",
    "\n",
    "        region_to_fit = feedme.header.region_to_fit\n",
    "        gname_df[\"crop_rad\"] = region_to_fit[1] - region_to_fit[0]\n",
    "\n",
    "    elif file_suffix.endswith(\".in\"):\n",
    "        feedme = FeedmeContainer(path_to_feedme = path_to_output)\n",
    "        feedme.from_file(path_to_output)\n",
    "        gname_df = feedme.to_pandas()\n",
    "\n",
    "        region_to_fit = feedme.header.region_to_fit\n",
    "        gname_df[\"crop_rad\"] = region_to_fit[1] - region_to_fit[0]\n",
    "\n",
    "    gname_df[\"label\"] = label #str_label\n",
    "    # TODO: Put this rename function in a to_pandas() function of OutputFits\n",
    "    # Make note in documentation: name is incoherent in everything but an outputfits\n",
    "    # since a feedme does not necessarily imply a galaxy\n",
    "    # That being said, make an optional gname parameter in feedme\n",
    "    gname_df.rename(index = {0:gname}, inplace = True)\n",
    "    \n",
    "    return gname_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bbf2c2-77a2-4ca8-bd78-c760ab2a129b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_angles(in_df, **kwargs):\n",
    "    angles = [\"position_angle_sersic_1\",\n",
    "              \"position_angle_sersic_2\",\n",
    "              \"cumul_rot_power_2\",\n",
    "              \"inclination_power_2\",\n",
    "              \"sky_position_angle_power_2\"]\n",
    "    \n",
    "    angles = kwargs.get(\"angles\", angles)\n",
    "    \n",
    "    out_df = in_df.copy()\n",
    "    \n",
    "    for col_name in angles:\n",
    "        if col_name in out_df.columns:\n",
    "            # Take advantage of symmetry across axes\n",
    "            # Will have to be careful when outputting new template to retain correct direction\n",
    "            # will likely pull this from original data\n",
    "            out_df.loc[out_df[col_name] < 0, col_name] += 180\n",
    "            out_df[col_name] *= np.pi/180\n",
    "            \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a11bb7-07c0-4972-8ff8-5cb0495a35bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_df(picklepath, label_dict, label_dirs, file_suffix):\n",
    "    if exists(picklepath):\n",
    "        df = pickle.load(open(picklepath, \"rb\"))\n",
    "    else:\n",
    "        if file_suffix.endswith(\".fits\"):\n",
    "            df = Parallel(n_jobs = -2, timeout = 30)(\n",
    "            delayed(build_df)(\n",
    "                              gname,\n",
    "                              label,\n",
    "                              count,\n",
    "                              label_dirs=label_dirs, \n",
    "                              file_suffix=\"_galfit_out.fits\"\n",
    "                             ) \n",
    "            for count, (gname, label) in enumerate(label_dict.items())\n",
    "                                                    )\n",
    "        elif file_suffix.endswith(\".in\"):\n",
    "            df = [build_df(gname, label, count, label_dirs, file_suffix) \n",
    "                  for count, (gname, label) in enumerate(label_dict.items())]   \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f\"file suffix {file_suffix} not valid.\")\n",
    "        \n",
    "        df = pd.concat(df)\n",
    "        \n",
    "        # For the input galaxies, we have a lot of held values, these are uneccessary\n",
    "        # https://stackoverflow.com/a/39658662\n",
    "        nunique = df.nunique()\n",
    "        cols_to_drop = nunique[nunique == 1].index\n",
    "        df.drop(columns = cols_to_drop, inplace = True)\n",
    "        \n",
    "        df.to_pickle(picklepath)\n",
    "\n",
    "    # Save *before* conversion for data posterity\n",
    "    df = convert_angles(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e89dd4-5ce5-4f3a-828c-999bdf915b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing GALFIT input data...\n",
      "Done!\n",
      "\n",
      "Grabbing GALFIT output data, this may take awhile (if not already saved)...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Grabbing GALFIT input data...\")\n",
    "    galfit_in_df = prepare_df(\"galfit_in_df.pkl\", label_dict, [not_good_inputs_dir, good_inputs_dir], \".in\")\n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    print(\"Grabbing GALFIT output data, this may take awhile (if not already saved)...\")\n",
    "    galfit_out_df = prepare_df(\"galfit_out_df.pkl\", label_dict, [not_good_outputs_dir, good_outputs_dir], \"_galfit_out.fits\")\n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04dac6a6-337d-4bbf-ad4c-44853be94aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     in_pkl = pj(cwd, \"galfit_in_df.pkl\")\n",
    "#     if exists(in_pkl):\n",
    "#         galfit_in_df = pickle.load(open(in_pkl, \"rb\"))\n",
    "#     else:   \n",
    "#         galfit_in_df = [build_df(gname, label, count, [not_good_inputs_dir, good_inputs_dir], \".in\") \n",
    "#                         for count, (gname, label) in enumerate(label_dict.items())]   \n",
    "        \n",
    "#         galfit_in_df = pd.concat(galfit_in_df)\n",
    "        \n",
    "#         # For the input galaxies, we have a lot of held values, these are uneccessary\n",
    "#         # https://stackoverflow.com/a/39658662\n",
    "#         nunique = galfit_in_df.nunique()\n",
    "#         cols_to_drop = nunique[nunique == 1].index\n",
    "#         galfit_in_df.drop(columns = cols_to_drop, inplace = True)\n",
    "        \n",
    "#         galfit_in_df.to_pickle(in_pkl)\n",
    "        \n",
    "#     galfit_in_df = convert_angles(galfit_in_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "607ff169-cddf-4ed3-a45b-0ad446754923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Parallelize this since FITS take so much longer\n",
    "# # I could alternatively output all the .in files from those FITS files separate and then build the DFs... a thought\n",
    "# if __name__ == \"__main__\":\n",
    "#     out_pkl = pj(cwd, \"galfit_out_df.pkl\")\n",
    "#     if exists(out_pkl):\n",
    "#         galfit_out_df = pickle.load(open(out_pkl, \"rb\"))\n",
    "#     else:\n",
    "#         galfit_out_df = Parallel(n_jobs = -2, timeout = 30)(\n",
    "#             delayed(parallel_build_df)(\n",
    "#                                        gname,\n",
    "#                                        label,\n",
    "#                                        count,\n",
    "#                                        label_dirs=[not_good_outputs_dir, good_outputs_dir], \n",
    "#                                        file_suffix=\"_galfit_out.fits\"\n",
    "#                                       ) \n",
    "#             for count, (gname, label) in enumerate(label_dict.items())\n",
    "#                                                                            )\n",
    "#         galfit_out_df = pd.concat(galfit_out_df)\n",
    "        \n",
    "#         # For the input galaxies, we have a lot of held values, these are uneccessary\n",
    "#         # https://stackoverflow.com/a/39658662\n",
    "#         nunique = galfit_out_df.nunique()\n",
    "#         cols_to_drop = nunique[nunique == 1].index\n",
    "#         galfit_out_df.drop(columns = cols_to_drop, inplace = True)\n",
    "        \n",
    "#         galfit_out_df.to_pickle(out_pkl)\n",
    "        \n",
    "#     galfit_out_df = convert_angles(galfit_out_df)\n",
    "#     #galfit_out_df['crop_rad'] = galfit_in_df['crop_rad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb439fc5-9c8f-4fa4-8692-981e4d4560fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check deprecated then delete\n",
    "# positions = [\"position_x_sersic_1\",\n",
    "#              \"position_y_sersic_1\",\n",
    "#              \"position_x_sersic_2\",\n",
    "#              \"position_y_sersic_2\"]\n",
    "\n",
    "# fourier = [\"F1_amplitude_fourier_2\",\n",
    "#            \"F1_phase_angle_fourier_2\",\n",
    "#            \"F3_amplitude_fourier_2\",\n",
    "#            \"F3_phase_angle_fourier_2\"\n",
    "#           ]\n",
    "\n",
    "# file_prefixes = [\"reduced_galfit_in\", \"reduced_galfit_out\"]\n",
    "# ignore_galfit_in  = [\"inner_rad_power_2\"]\n",
    "# ignore_galfit_in.extend(positions)\n",
    "# ignore_galfit_in.extend(fourier)\n",
    "\n",
    "# ignore_galfit_out = [\"crop_rad\",\n",
    "#                      \"effective_radius_sersic_1\", # Bulge radius goes crazzyyyyyy\n",
    "#                      \"inclination_power_2\", # Trust sparc/galfit on this one\n",
    "#                      \"inner_rad_power_2\",\n",
    "#                      \"outer_rad_power_2\",\n",
    "#                      \"sky_background_sky_3\", # Trust galfit on this one\n",
    "#                      \"dsky_dx_sky_3\", # if we include them, does it get better? for future testing\n",
    "#                      \"dsky_dy_sky_3\"\n",
    "#                     ]\n",
    "# ignore_galfit_out.extend(positions)\n",
    "# ignore_galfit_out.extend(fourier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# in_filter = []\n",
    "# # filter_1 = \"sersic_index_sersic_1\"\n",
    "# # filter_2 = \"sersic_index_sersic_1\"\n",
    "# # filter_3 = \"magnitude_sersic_1\"\n",
    "# # filter_4 = \"effective_radius_sersic_1\"\n",
    "# # filter_5 = \"effective_radius_sersic_2\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb03153d-6dea-45b4-9275-c6c828adbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_filter():\n",
    "    positions = [\"position_x_sersic_1\",\n",
    "                 \"position_y_sersic_1\",\n",
    "                 \"position_x_sersic_2\",\n",
    "                 \"position_y_sersic_2\"]\n",
    "\n",
    "    fourier = [\"F1_amplitude_fourier_2\",\n",
    "               \"F1_phase_angle_fourier_2\",\n",
    "               \"F3_amplitude_fourier_2\",\n",
    "               \"F3_phase_angle_fourier_2\"\n",
    "              ]\n",
    "    \n",
    "    # Not all df may have columns I'd otherwise check\n",
    "    # For instance, I hold the input constant for some variables (sersic index) so the input\n",
    "    # df won't have those columns. So be careful here\n",
    "    ignore_galfit_in  = [\"inner_rad_power_2\"]\n",
    "    ignore_galfit_in.extend(positions)\n",
    "    ignore_galfit_in.extend(fourier)\n",
    "    \n",
    "    ignore_galfit_out = [\"crop_rad\",\n",
    "                         \"effective_radius_sersic_1\", # Bulge radius goes crazzyyyyyy\n",
    "                         \"inclination_power_2\", # Trust sparc/galfit on this one\n",
    "                         \"inner_rad_power_2\",\n",
    "                         \"outer_rad_power_2\",\n",
    "                         \"F1_amplitude_fourier_2\",\n",
    "                         \"F3_amplitude_fourier_2\",\n",
    "                         \"F1_phase_angle_fourier_2\",\n",
    "                         \"F3_phase_angle_fourier_2\",\n",
    "                         \"sky_background_sky_3\", # Trust galfit on this one\n",
    "                         \"dsky_dx_sky_3\", # if we include them, does it get better? for future testing\n",
    "                         \"dsky_dy_sky_3\"\n",
    "                        ]\n",
    "    ignore_galfit_out.extend(positions)\n",
    "    ignore_galfit_out.extend(fourier)\n",
    "\n",
    "    in_filter = []\n",
    "    \n",
    "    out_filter = [#\"`Asymptotic spiral powerlaw disk` <= 1\",\n",
    "              f\"`sersic_index_sersic_1` > 14\",\n",
    "              f\"`sersic_index_sersic_1` < 0.05\",\n",
    "              f\"`magnitude_sersic_1` > 26\",\n",
    "              f\"`effective_radius_sersic_1` > `crop_rad`\",\n",
    "              f\"`effective_radius_sersic_2` > `crop_rad`\"\n",
    "              #\"`R_e (effective radius)   (pix) bulge` < `Crop Rad`\"\n",
    "              ]\n",
    "    \n",
    "    return ignore_galfit_in, ignore_galfit_out, in_filter, out_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c72bfb-04f8-48f0-9cc9-6423712b15f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # in_train/test should always have fewer exclusions and those exclusions must also be in the out_train/test\n",
    "    # so that the algo isn't predicting things sight unseen\n",
    "    sklearn = True\n",
    "\n",
    "    ignore_galfit_in, ignore_galfit_out, in_filter, out_filter = export_filter()\n",
    "    \n",
    "    file_prefixes = [\"reduced_galfit_in\", \"reduced_galfit_out\"]\n",
    "    \n",
    "    ignore_galfit = (ignore_galfit_in, ignore_galfit_out)\n",
    "    col_ignore = {fp:ig for fp, ig in zip(file_prefixes, ignore_galfit)}\n",
    "    \n",
    "    filter_strings = (in_filter, out_filter)\n",
    "    gal_filter = {fp:fs for fp, fs in zip(file_prefixes, filter_strings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dbcaf11-c6f0-4e63-8504-9d74c3549b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per https://xgboost.readthedocs.io/en/stable/python/python_intro.html\n",
    "# For training *regressor* on good data\n",
    "def split_save_df_reg(*args, file_prefixes = [], col_ignore = {}, gal_filter = {}, sklearn = False):\n",
    "    # col_ignore must be list of list of columns since we drop different things for different df\n",
    "    # gal_filter must be given as: `column name` cond value, i.e. \"`Asymptotic spiral powerlaw disk` <= 1\"\n",
    "    # Note, the conditions are what we *don't want*\n",
    "    \n",
    "    assert len(file_prefixes) == len(args), \"File prefixes must be same length as # of dataframes being passed in! Try again\"\n",
    "    #assert col_ignore == len(args), \"col_ignore must be same length as # of dataframes being passed in! Try again\"\n",
    "    \n",
    "    return_dict = {}\n",
    "    \n",
    "    # Specifically looking for good galaxies since we're not classifying\n",
    "    #gal_filter.append(\"label != 1\") \n",
    "    #gal_filter = \" or \".join(gal_filter)\n",
    "    \n",
    "    galaxies_to_drop = [in_df.query(\n",
    "                                    \" or \".join(gf + [\"label != 1\"])\n",
    "                                    ).index \n",
    "                        for in_df, gf in zip(args, gal_filter.values()) if gf]\n",
    "    \n",
    "    # Unpacking to keep the list comp ;)\n",
    "    galaxies_to_drop = list(itertools.chain.from_iterable(galaxies_to_drop))\n",
    "    print(f\"Keeping {len(args[0]) - len(galaxies_to_drop)} galaxies (out of {len(args[0].query('label == 1'))})\")\n",
    "    \n",
    "    for in_df, file_prefix in zip(args, file_prefixes):\n",
    "        # Filter and exclude\n",
    "        \n",
    "        exclude = ['label'] + col_ignore.get(file_prefix, [])\n",
    "        \n",
    "        exclude = list(set(exclude).intersection(set(in_df.columns)))\n",
    "        in_df_good = in_df.drop(index = galaxies_to_drop, columns = exclude)\n",
    "        \n",
    "        label = pd.DataFrame(np.ones(len(in_df_good)), columns = [\"label\"], dtype = \"int\")\n",
    "            \n",
    "        # Pass random_state = 0 to guarantee input and output are lined up\n",
    "        X_train, X_test, y_train, y_test = train_test_split(in_df_good, label, test_size=.3, random_state = 0)\n",
    "    \n",
    "#         dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "#         dtest  = xgb.DMatrix(X_test,  label = y_test)\n",
    "#         ddata  = xgb.DMatrix(in_df_good, label = label)\n",
    "\n",
    "#         print(f\"Saving dmatrices to file: {file_prefix}.train/test/data\")\n",
    "#         dtrain.save_binary(f'{file_prefix}.train')\n",
    "#         dtest.save_binary(f'{file_prefix}.test')\n",
    "#         ddata.save_binary(f'{file_prefix}.data')\n",
    "        \n",
    "        in_df_good[\"label\"] = list(label.label)\n",
    "        if sklearn:\n",
    "            return_dict[file_prefix] = X_train, X_test, y_train, y_test, in_df_good\n",
    "        else:\n",
    "            return_dict[file_prefix] = dtrain, dtest, ddata, in_df_good, label\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07e7d562-2a85-402b-830b-9b25e4d8ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: FILTER OUT PREVIOUS GALFIT RESULTS FOR NON-PHYSICAL DATA, I.E. SERSIC INDEX TOO HIGH OR TOO LOW IN GOOD DATA\n",
    "# Convert angles to radians -- done\n",
    "# weight spiral law(?), look up how weighting works -- doesn't work\n",
    "# Make better plots (seaborn?)\n",
    "# Use physics based constraints to filter results\n",
    "# Determine better loss method? Hmmmmm https://stats.stackexchange.com/a/445454\n",
    "# For +/- things (like angle) find some way to use absolute value while retaining distribution, see if that makes things better -- done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad28c5c7-1e56-4faa-b0b0-a4179c8cf621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 261 galaxies (out of 304)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    split_save_out = split_save_df_reg(\n",
    "                                       galfit_in_df, galfit_out_df,\n",
    "                                       file_prefixes = file_prefixes,\n",
    "                                       col_ignore    = col_ignore,\n",
    "                                       gal_filter    = gal_filter,\n",
    "                                       sklearn       = sklearn\n",
    "                                      )\n",
    "\n",
    "    # New...df is a combo of train/test, no different otherwise\n",
    "    in_train,  in_test,  _, _, new_in_df    = split_save_out[file_prefixes[0]]\n",
    "    out_train, out_test, _, _, new_out_df   = split_save_out[file_prefixes[1]]\n",
    "\n",
    "    # Could train on bad galaxies too... but only for classification purposes\n",
    "    \n",
    "    assert len(galfit_out_df) - len(galfit_out_df.query(\" or \".join(out_filter + [\"label != 1\"]))) == len(new_out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e3313c-1c59-43cb-a277-5c68610a23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_prefix):\n",
    "    dtrain = xgb.DMatrix(f'{file_prefix}.train')\n",
    "    dtest  = xgb.DMatrix(f'{file_prefix}.test')\n",
    "    ddata  = xgb.DMatrix(f'{file_prefix}.data')\n",
    "    \n",
    "    return dtrain, dtest, ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa01f715-0e93-4fca-9736-e35555347b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     if not sklearn:\n",
    "#         dtrain_in, dtest_in, ddata_in = load_data(file_prefixes[0])\n",
    "#         dtrain_out, dtest_out, ddata_out = load_data(file_prefixes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef6e963-6563-4ddb-84e6-48d60ec8d6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_methods(test_df, pred_df, method = \"kstest\"):\n",
    "    method = method.lower()\n",
    "    \n",
    "    if method == \"kstest\":\n",
    "        loss = np.average([kstest(test_df[col].values, pred_df[col].values).statistic\n",
    "                                      for col in pred_df])\n",
    "        method_str = \"KStest Stat\"\n",
    "\n",
    "\n",
    "    elif method == \"rmse\":\n",
    "        # squared = False gives RMSE\n",
    "        loss = MSE(test_df, pred_df, squared = False)\n",
    "        method_str = \"RMSE\"\n",
    "        \n",
    "    # Average of per galaxy rmse\n",
    "    elif method == \"galaxy_rmse\":\n",
    "        comparison_df = np.square(test_df - pred_df)\n",
    "        loss = np.mean(np.sqrt(comparison_df.mean(axis=1)))\n",
    "        method_str = \"Per Galaxy RMSE\"\n",
    "        \n",
    "    print(f\"Average {method_str}: {loss:.3f}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ddb1043-7594-4e46-ac66-4f24cc86ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def objective(space): #, data, label, test_size = 0.3):\n",
    "    clf = xgb.XGBRegressor(\n",
    "                    learning_rate = space['learning_rate'],\n",
    "                    n_estimators = space['n_estimators'],\n",
    "                    #subsample = space['subsample'],\n",
    "                    max_depth = int(space['max_depth']))\n",
    "                    #gamma = space['min_split_loss'])\n",
    "                    #reg_alpha = int(space['reg_alpha']),\n",
    "                    #min_child_weight=int(space['min_child_weight']),\n",
    "                    #colsample_bytree=int(space['colsample_bytree']))\n",
    "    \n",
    "    clf.set_params(#eval_metric=\"auc\",\n",
    "                   tree_method = \"hist\",\n",
    "                   early_stopping_rounds=10,\n",
    "                   objective='reg:squarederror'\n",
    "                   #objective='reg:pseudohubererror'\n",
    "                   #'eval_metric' : 'error' # Binary classification error rate\n",
    "                   )\n",
    "    \n",
    "    clf.fit(in_train, \n",
    "            out_train, \n",
    "            eval_set = [(in_train, out_train)],\n",
    "            verbose = False)\n",
    "\n",
    "    pred = clf.predict(in_test)\n",
    "    out_pred_df = pd.DataFrame(pred, columns = out_test.columns, index = in_test.index)\n",
    "\n",
    "    # use weights to lower contribution from values with wider spread since these aren't all normalized\n",
    "    # alternatively normalize everything to [0,1]\n",
    "    #rmse = np.sqrt(MSE(out_test, pred))#, multioutput = \"raw_values\"))\n",
    "    #print(f\"RMSE: {rmse:.3f}\")\n",
    "    \n",
    "#     acc = []\n",
    "#     for col in out_test:\n",
    "#         acc.append(kstest(out_test[col].values, out_pred_df[col].values).statistic)\n",
    "        \n",
    "#     print(np.average(acc))\n",
    "        \n",
    "    loss = loss_methods(out_test, out_pred_df, method = \"kstest\")\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK }\n",
    "    #accuracy = accuracy_score(y_test, pred>0.5)\n",
    "    #print (f\"SCORE: {accuracy:.3f}\")\n",
    "    #return {'loss': -accuracy, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c76a6a62-06ab-4841-8f8a-390b42749e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KStest Stat: 0.362                             \n",
      "Average KStest Stat: 0.278                                                       \n",
      "Average KStest Stat: 0.207                                                        \n",
      "Average KStest Stat: 0.200                                                        \n",
      "Average KStest Stat: 0.222                                                        \n",
      "Average KStest Stat: 0.437                                                        \n",
      "Average KStest Stat: 0.234                                                        \n",
      "Average KStest Stat: 0.216                                                        \n",
      "Average KStest Stat: 0.393                                                        \n",
      "Average KStest Stat: 0.205                                                        \n",
      "Average KStest Stat: 0.289                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "Average KStest Stat: 0.320                                                         \n",
      "Average KStest Stat: 0.265                                                         \n",
      "Average KStest Stat: 0.276                                                         \n",
      "Average KStest Stat: 0.206                                                         \n",
      "Average KStest Stat: 0.416                                                         \n",
      "Average KStest Stat: 0.442                                                         \n",
      "Average KStest Stat: 0.217                                                         \n",
      "Average KStest Stat: 0.227                                                         \n",
      "Average KStest Stat: 0.216                                                         \n",
      "Average KStest Stat: 0.205                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.206                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.199                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "Average KStest Stat: 0.207                                                         \n",
      "Average KStest Stat: 0.214                                                         \n",
      "Average KStest Stat: 0.222                                                         \n",
      "Average KStest Stat: 0.205                                                         \n",
      "Average KStest Stat: 0.306                                                         \n",
      "Average KStest Stat: 0.208                                                         \n",
      "Average KStest Stat: 0.487                                                         \n",
      "Average KStest Stat: 0.205                                                         \n",
      "Average KStest Stat: 0.210                                                         \n",
      "Average KStest Stat: 0.293                                                         \n",
      "Average KStest Stat: 0.206                                                         \n",
      "Average KStest Stat: 0.318                                                         \n",
      "Average KStest Stat: 0.247                                                         \n",
      "Average KStest Stat: 0.207                                                         \n",
      "Average KStest Stat: 0.209                                                         \n",
      "Average KStest Stat: 0.592                                                         \n",
      "Average KStest Stat: 0.209                                                         \n",
      "Average KStest Stat: 0.259                                                         \n",
      "Average KStest Stat: 0.208                                                         \n",
      "Average KStest Stat: 0.223                                                         \n",
      "Average KStest Stat: 0.244                                                         \n",
      "Average KStest Stat: 0.306                                                         \n",
      "Average KStest Stat: 0.210                                                         \n",
      "Average KStest Stat: 0.213                                                         \n",
      "Average KStest Stat: 0.216                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "Average KStest Stat: 0.224                                                         \n",
      "Average KStest Stat: 0.688                                                         \n",
      "Average KStest Stat: 0.199                                                         \n",
      "Average KStest Stat: 0.332                                                         \n",
      "Average KStest Stat: 0.216                                                         \n",
      "Average KStest Stat: 0.199                                                         \n",
      "Average KStest Stat: 0.197                                                         \n",
      "Average KStest Stat: 0.213                                                         \n",
      "Average KStest Stat: 0.207                                                        \n",
      "Average KStest Stat: 0.201                                                        \n",
      "Average KStest Stat: 0.200                                                        \n",
      "Average KStest Stat: 0.196                                                        \n",
      "Average KStest Stat: 0.200                                                         \n",
      "Average KStest Stat: 0.205                                                         \n",
      "Average KStest Stat: 0.207                                                         \n",
      "Average KStest Stat: 0.203                                                         \n",
      "Average KStest Stat: 0.200                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "Average KStest Stat: 0.197                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.209                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.199                                                         \n",
      "Average KStest Stat: 0.203                                                         \n",
      "Average KStest Stat: 0.206                                                         \n",
      "Average KStest Stat: 0.204                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "Average KStest Stat: 0.209                                                         \n",
      "Average KStest Stat: 0.207                                                         \n",
      "Average KStest Stat: 0.203                                                         \n",
      "Average KStest Stat: 0.196                                                         \n",
      "Average KStest Stat: 0.217                                                         \n",
      "Average KStest Stat: 0.208                                                         \n",
      "Average KStest Stat: 0.206                                                         \n",
      "Average KStest Stat: 0.210                                                         \n",
      "Average KStest Stat: 0.213                                                         \n",
      "Average KStest Stat: 0.262                                                         \n",
      "Average KStest Stat: 0.228                                                         \n",
      "Average KStest Stat: 0.197                                                         \n",
      "Average KStest Stat: 0.239                                                         \n",
      "Average KStest Stat: 0.286                                                         \n",
      "Average KStest Stat: 0.217                                                         \n",
      "Average KStest Stat: 0.209                                                         \n",
      "Average KStest Stat: 0.205                                                         \n",
      "Average KStest Stat: 0.207                                                         \n",
      "Average KStest Stat: 0.212                                                         \n",
      "100%|| 100/100 [02:49<00:00,  1.69s/trial, best loss: 0.19620253164556958]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.19966693827808601, 'max_depth': 16, 'n_estimators': 96}\n"
     ]
    }
   ],
   "source": [
    "# Optimizing hyperparameters using hyperopt???\n",
    "# Bayesian Optimization\n",
    "if __name__ == \"__main__\":\n",
    "    space = {'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "             'max_depth': hp.quniform('max_depth', 2, 18, 1),\n",
    "             #'subsample' : hp.uniform('subsample', 0.5, 1),\n",
    "             #'min_child_weight' : hp.quniform('min_child_weight', 0, 5, 1),\n",
    "             #'max_delta_step' : hp.quniform('max_delta_step', 0, 10, 1),\n",
    "             #'min_split_loss': hp.uniform ('min_split_loss', 1,9),\n",
    "             #'reg_alpha' : hp.quniform('reg_alpha', 40, 180,1),\n",
    "             #'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "             #'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "             #'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "             'n_estimators' : hp.uniformint('n_estimators', 5, 100),\n",
    "             'seed': 0\n",
    "            }\n",
    "    \n",
    "    trials = Trials()\n",
    "\n",
    "    data = new_in_df.drop(columns = \"label\")\n",
    "    label = new_in_df[\"label\"]\n",
    "\n",
    "    best_hyperparams = fmin(fn = objective,\n",
    "                            space = space,\n",
    "                            algo = tpe.suggest,\n",
    "                            max_evals = 100,\n",
    "                            trials = trials\n",
    "                           )\n",
    "    \n",
    "    best_hyperparams_int = {k:(int(v) if int(float(v)) == v else v) for k,v in best_hyperparams.items()}       \n",
    "    print(\"The best hyperparameters are : \",\"\\n\")\n",
    "    print(best_hyperparams_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdebef-de04-4e46-a402-5f22a90e4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://davetang.org/muse/2012/04/17/comparing-different-distributions/\n",
    "# https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html\n",
    "# or https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n",
    "# ?\n",
    "# Also https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\n",
    "# https://asaip.psu.edu/articles/beware-the-kolmogorov-smirnov-test/\n",
    "# OK Darling compares against a known distribution so that won't work ha(!)\n",
    "# The null hypothesis is that both samples come from the same distribution and is not rejected (p-value = 0.5361) since they do come from the exact same distribution.\n",
    "\n",
    "# We can try making this the metric but according to this answer on stackexchange https://stats.stackexchange.com/a/511714\n",
    "# we might be better served leaving it as MSE (especially if it's not differentiable) and use that for evaluation only\n",
    "#scipy.stats.kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a160a2-6a0a-4c4d-b77a-350b62d4e183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Eval\n",
      "magnitude_sersic_1: 1.5563\n",
      "sersic_index_sersic_1: 1.9373\n",
      "axis_ratio_sersic_1: 0.2646\n",
      "position_angle_sersic_1: 55.4300\n",
      "magnitude_sersic_2: 0.8461\n",
      "effective_radius_sersic_2: 7.0303\n",
      "sersic_index_sersic_2: 0.6772\n",
      "axis_ratio_sersic_2: 0.1891\n",
      "position_angle_sersic_2: 58.7081\n",
      "cumul_rot_power_2: 206.5808\n",
      "powerlaw_power_2: 9.3026\n",
      "sky_position_angle_power_2: 56.8908\n",
      "\n",
      "KSTest eval\n",
      "magnitude_sersic_1\n",
      "KstestResult(statistic=0.10126582278481013, pvalue=0.816050726346802, statistic_location=13.8839, statistic_sign=1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "sersic_index_sersic_1\n",
      "KstestResult(statistic=0.17721518987341772, pvalue=0.16769086609413486, statistic_location=0.68595344, statistic_sign=-1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "axis_ratio_sersic_1\n",
      "KstestResult(statistic=0.27848101265822783, pvalue=0.004190220170180564, statistic_location=0.7048072, statistic_sign=-1)\n",
      "Significant: Test and prediction distributions differ significantly.\n",
      "\n",
      "position_angle_sersic_1\n",
      "KstestResult(statistic=0.22784810126582278, pvalue=0.032783641954619344, statistic_location=0.80108863, statistic_sign=1)\n",
      "Significant: Test and prediction distributions differ significantly.\n",
      "\n",
      "magnitude_sersic_2\n",
      "KstestResult(statistic=0.12658227848101267, pvalue=0.5542881699689268, statistic_location=13.689334, statistic_sign=-1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "effective_radius_sersic_2\n",
      "KstestResult(statistic=0.11392405063291139, pvalue=0.6878282087774076, statistic_location=21.4364, statistic_sign=1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "sersic_index_sersic_2\n",
      "KstestResult(statistic=0.31645569620253167, pvalue=0.000673273747704539, statistic_location=0.5738, statistic_sign=1)\n",
      "Significant: Test and prediction distributions differ significantly.\n",
      "\n",
      "axis_ratio_sersic_2\n",
      "KstestResult(statistic=0.22784810126582278, pvalue=0.032783641954619344, statistic_location=0.7459345, statistic_sign=-1)\n",
      "Significant: Test and prediction distributions differ significantly.\n",
      "\n",
      "position_angle_sersic_2\n",
      "KstestResult(statistic=0.22784810126582278, pvalue=0.032783641954619344, statistic_location=2.340417, statistic_sign=-1)\n",
      "Significant: Test and prediction distributions differ significantly.\n",
      "\n",
      "cumul_rot_power_2\n",
      "KstestResult(statistic=0.17721518987341772, pvalue=0.16769086609413486, statistic_location=0.26674035, statistic_sign=1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "powerlaw_power_2\n",
      "KstestResult(statistic=0.13924050632911392, pvalue=0.43023321410525717, statistic_location=-0.63962704, statistic_sign=-1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "sky_position_angle_power_2\n",
      "KstestResult(statistic=0.17721518987341772, pvalue=0.16769086609413486, statistic_location=2.4380987, statistic_sign=-1)\n",
      "Insignificant: Test and prediction distributions do not differ signficantly.\n",
      "\n",
      "7/12 prediction distributions match within p < 0.05\n"
     ]
    }
   ],
   "source": [
    "# Regresssion\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If chosen by hand\n",
    "    param = {\"tree_method\"   : \"hist\",\n",
    "             \"n_estimators\"  : 120, \n",
    "             \"max_depth\"     : 6, \n",
    "             \"learning_rate\" : 0.15}\n",
    "             #\"objective\"     : \"binary:logistic\"}\n",
    "\n",
    "    reg = xgb.XGBRegressor(#**param\n",
    "                           **best_hyperparams_int\n",
    "                           )\n",
    "                           \n",
    "    reg.fit(in_train, \n",
    "            out_train, \n",
    "            eval_set = [(in_train, out_train)],\n",
    "            verbose = False)\n",
    "\n",
    "    reg_pred = reg.predict(in_test)\n",
    "    out_pred_df = pd.DataFrame(reg_pred, \n",
    "                               columns = out_test.columns, \n",
    "                               index = in_test.index\n",
    "                              )\n",
    "\n",
    "    # squared = False gives RMSE\n",
    "    rmse = MSE(out_test, \n",
    "               reg_pred, \n",
    "               multioutput = \"raw_values\", \n",
    "               squared = False\n",
    "              )\n",
    "\n",
    "    print(\"RMSE Eval\")\n",
    "    for col, score in zip(out_train.columns, rmse):\n",
    "        if \"cumul_rot\" in col or \"position_angle\" in col.lower():\n",
    "            score *= 180/np.pi\n",
    "        print(f\"{col}: {score:.4f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"KSTest eval\")\n",
    "    # Null hypothesis -- the samples are pulled from the same distribution\n",
    "    # typical choice is <0.05, reject null hypothesis\n",
    "    kstest_results = {col : kstest(out_test[col].values, out_pred_df[col].values)\n",
    "                      for col in out_pred_df}\n",
    "\n",
    "    insig_count = 0\n",
    "    for col, result in kstest_results.items():\n",
    "        print(col)\n",
    "        print(result)\n",
    "        if result.pvalue < 0.05:\n",
    "            result_str = \"Significant: Test and prediction distributions differ significantly.\"\n",
    "        else:\n",
    "            result_str = \"Insignificant: Test and prediction distributions do not differ signficantly.\"\n",
    "            insig_count += 1\n",
    "        print(result_str)\n",
    "        print()\n",
    "    print(f\"{insig_count}/{len(kstest_results)} prediction distributions match within p < 0.05\")\n",
    "\n",
    "    #plot_predt(out_train.to_numpy(), reg_pred, \"multi\")\n",
    "    # Closer to 1 is better \n",
    "    # https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.score\n",
    "    # I'm thinking R^2 isn't a good test here, just looking at the data\n",
    "    # reg.score(in_test, out_test)\n",
    "\n",
    "    # TODO: Can we average the difference of every parameter per galaxy and use that to evaluate the model on a per galaxy basis?\n",
    "    # With some weighting I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af38fbfd-878d-48a1-ac4f-57ca797cf888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anderson_darling(in_df):\n",
    "    # Null hypothesis -- the samples are pulled from the normal distribution\n",
    "    # statistic is the 'result', critical_values are the values at which we can check each\n",
    "    # significance level specified by significance_level\n",
    "    # The statistic must be greater than a critical value to determine a significance value\n",
    "    #\n",
    "    # ex: statistic = 0.8\n",
    "    # significance array (%) = [15. , 10. ,  5. ,  2.5,  1. ]\n",
    "    # critical values        = [0.552, 0.629, 0.755, 0.88 , 1.047]\n",
    "    # Then the result is significant at the 5%/0.05 confidence level\n",
    "    \n",
    "    anderson_results = {col : anderson(in_df[col].values, dist = 'norm')\n",
    "                        for col in in_df}\n",
    "    \n",
    "    crit_sig = dict(zip(list(anderson_results.values())[0].critical_values, \n",
    "                        list(anderson_results.values())[0].significance_level))\n",
    "    \n",
    "    cv = list(crit_sig.keys())\n",
    "    sl = list(crit_sig.values())\n",
    "    \n",
    "    print(f\"Critical Values: {cv}\")\n",
    "    print(f\"Significance Levels: {sl}\")\n",
    "    print()\n",
    "    \n",
    "    for col, result in anderson_results.items():\n",
    "        rs = result.statistic\n",
    "        # Checking highest crit val the significance is greater than\n",
    "        cv_filter = list(filter(lambda x: rs > x, cv))\n",
    "        if cv_filter:\n",
    "            significance_value = crit_sig[cv_filter[-1]]\n",
    "        else:\n",
    "            significance_value = \"N/A\"\n",
    "            \n",
    "        print(f\"{col}\\n{rs}, satisfies {significance_value} confidence level\\n\", sep = \"\")\n",
    "    \n",
    "    return anderson_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50c0f52f-e301-4528-b5a8-a51cd3341087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anderson-Darling eval of prediction (vs Normal Distribution)\n",
      "Critical Values: [0.55, 0.627, 0.752, 0.877, 1.043]\n",
      "Significance Levels: [15.0, 10.0, 5.0, 2.5, 1.0]\n",
      "\n",
      "magnitude_sersic_1\n",
      "0.3379822872302043, satisfies N/A confidence level\n",
      "\n",
      "sersic_index_sersic_1\n",
      "13.381663129794262, satisfies 1.0 confidence level\n",
      "\n",
      "axis_ratio_sersic_1\n",
      "1.9512935148939619, satisfies 1.0 confidence level\n",
      "\n",
      "position_angle_sersic_1\n",
      "0.6886941780897473, satisfies 10.0 confidence level\n",
      "\n",
      "magnitude_sersic_2\n",
      "1.0955898035035432, satisfies 1.0 confidence level\n",
      "\n",
      "effective_radius_sersic_2\n",
      "1.9438571326921164, satisfies 1.0 confidence level\n",
      "\n",
      "sersic_index_sersic_2\n",
      "1.4037887654092032, satisfies 1.0 confidence level\n",
      "\n",
      "axis_ratio_sersic_2\n",
      "0.547236267077551, satisfies N/A confidence level\n",
      "\n",
      "position_angle_sersic_2\n",
      "0.19540819520564412, satisfies N/A confidence level\n",
      "\n",
      "cumul_rot_power_2\n",
      "0.16842935699413886, satisfies N/A confidence level\n",
      "\n",
      "powerlaw_power_2\n",
      "1.9227335040144027, satisfies 1.0 confidence level\n",
      "\n",
      "sky_position_angle_power_2\n",
      "1.5373365433071484, satisfies 1.0 confidence level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ************************************************************************\n",
    "# Should we expect these subsets to be normal?\n",
    "# ************************************************************************\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nAnderson-Darling eval of prediction (vs Normal Distribution)\")\n",
    "    _ = anderson_darling(out_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a15ffaa4-495b-4dcf-9866-0ed50bdb96d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anderson-Darling eval of test output (vs Normal Distribution)\n",
      "Critical Values: [0.55, 0.627, 0.752, 0.877, 1.043]\n",
      "Significance Levels: [15.0, 10.0, 5.0, 2.5, 1.0]\n",
      "\n",
      "magnitude_sersic_1\n",
      "0.8684949142020599, satisfies 5.0 confidence level\n",
      "\n",
      "sersic_index_sersic_1\n",
      "16.053461118345282, satisfies 1.0 confidence level\n",
      "\n",
      "axis_ratio_sersic_1\n",
      "1.8040859071427064, satisfies 1.0 confidence level\n",
      "\n",
      "position_angle_sersic_1\n",
      "0.856704009057637, satisfies 5.0 confidence level\n",
      "\n",
      "magnitude_sersic_2\n",
      "2.1036483954409135, satisfies 1.0 confidence level\n",
      "\n",
      "effective_radius_sersic_2\n",
      "1.6017656710096304, satisfies 1.0 confidence level\n",
      "\n",
      "sersic_index_sersic_2\n",
      "3.069565618094785, satisfies 1.0 confidence level\n",
      "\n",
      "axis_ratio_sersic_2\n",
      "1.9636298127546468, satisfies 1.0 confidence level\n",
      "\n",
      "position_angle_sersic_2\n",
      "1.3448192063788156, satisfies 1.0 confidence level\n",
      "\n",
      "cumul_rot_power_2\n",
      "4.198073905408961, satisfies 1.0 confidence level\n",
      "\n",
      "powerlaw_power_2\n",
      "19.279376907144766, satisfies 1.0 confidence level\n",
      "\n",
      "sky_position_angle_power_2\n",
      "1.1671753582390352, satisfies 1.0 confidence level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ************************************************************************\n",
    "# Could be used for interesting statistics once I'm actually confident in the algorithm ;)\n",
    "# ************************************************************************\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nAnderson-Darling eval of test output (vs Normal Distribution)\")\n",
    "    _ = anderson_darling(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea47991f-524c-43fb-9efb-77cf1f0fede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist_plots(test_data, predicted_data, grid = False, bins = 30, adjust = 2.17, save = False):\n",
    "    # Assume both test_data, predicted_data are pandas df\n",
    "    # Number of bins arrived at empirically\n",
    "\n",
    "    rows = len(test_data.columns)\n",
    "    cols = 1\n",
    "    \n",
    "    if grid:\n",
    "        rows = ceil(rows / 2)\n",
    "        # Restricting to 2 for now since histograms are wide\n",
    "        cols = 2\n",
    "        \n",
    "    fig = make_subplots(rows = rows, cols = cols, start_cell=\"top-left\") \n",
    "\n",
    "    for i, col_name in enumerate(predicted_data.columns): \n",
    "\n",
    "        # plotly indexes at 1 \n",
    "        if grid:\n",
    "            row = ceil((i + 1)/rows) \n",
    "            col = i % cols + 1 \n",
    "        else:\n",
    "            row = i + 1\n",
    "            col = 1    \n",
    "\n",
    "        #print(row, col) \n",
    "\n",
    "        fig.add_trace(go.Histogram(x = test_data[col_name],\n",
    "                                   nbinsx = bins,\n",
    "                                   name = \"Data\",\n",
    "                                   legendgroup = col_name,\n",
    "                                   legendgrouptitle_text = col_name,\n",
    "                                   marker_color = '#FFA15A'), #col_name + \" test\"), \n",
    "                      row = row, col = col) \n",
    "        \n",
    "        fig.add_trace(go.Histogram(x = predicted_data[col_name],\n",
    "                                   nbinsx = bins,\n",
    "                                   name = \"Predicted\",\n",
    "                                   legendgroup = col_name,\n",
    "                                   legendgrouptitle_text = col_name,\n",
    "                                   marker_color = 'cornflowerblue'), #col_name + \" pred\"), \n",
    "                      row = row, col = col)\n",
    "\n",
    "        fig.update_layout(barmode='overlay')#,\n",
    "                          #xaxis_title_text = col_name)\n",
    "            \n",
    "        # if save:\n",
    "        #     filepath = pj(cwd, col_name.replace(\" \", \"_\"))\n",
    "        #     fig.write_image(f\"{filepath}.svg\")\n",
    "\n",
    "    fig.update_traces(opacity = 0.6)\n",
    "    height = rows*200\n",
    "    width = 800\n",
    "    # For legend placement\n",
    "    # higher, less distance, lower, more distance\n",
    "    adjust = adjust\n",
    "    fig.update_layout(height = height, width = width,\n",
    "                      legend_tracegroupgap = height / (adjust*len(in_test.columns)))\n",
    "    \n",
    "    if save:\n",
    "        filepath = pj(os.getcwd, \"hist_plots\", \"all_plots\")\n",
    "        fig.write_image(f\"{filepath}.svg\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64d77aae-3fb7-44ad-90b2-64728f96c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    make_hist_plots(out_test, out_pred_df, adjust=1.83, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6ef9579-cb19-4ec2-b498-6fbdfb541b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reg.save_model(pj(cwd, \"xgboost_model.json\"))\n",
    "    # reg.load_model(\"xgboost_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdab6f5-9dde-439b-b0fd-7d5a43851612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting xgboost_train.ipynb\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    export_to_py(\"xgboost_train\", pj(_MODULE_DIR, \"XGBoost\", \"xgboost_train\"))\n",
    "    #export_to_py(\"xgboost_train\", \"xgboost_train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
